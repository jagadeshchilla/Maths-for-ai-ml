
# Linear Algebra Concepts (Comprehensive Guide)

This document provides a comprehensive overview of linear algebra concepts with detailed explanations, mathematical formulations, examples, and applications.

---

## 1ï¸âƒ£ Vectors in Linear Algebra

A **vector** is an ordered collection of numbers representing a point, direction, or quantity in space.

### Definition
A vector **v** in â„â¿ is an ordered n-tuple:

```
v = [vâ‚, vâ‚‚, ..., vâ‚™]áµ€ = (vâ‚, vâ‚‚, ..., vâ‚™)
```

**Column vector notation:**
```
v = [vâ‚]
    [vâ‚‚]
    [â‹®]
    [vâ‚™]
```

### Basic Properties
- **Notation:** **v** = [vâ‚, vâ‚‚, ..., vâ‚™] or **v** = (vâ‚, vâ‚‚, ..., vâ‚™)
- **Dimension:** number of components (2D, 3D, n-dimensional)
- **Geometric meaning:** points to a direction and has magnitude
- **Zero vector:** **0** = [0, 0, ..., 0]áµ€

### Vector Operations

#### Addition
For vectors **v** = [vâ‚, vâ‚‚, ..., vâ‚™]áµ€ and **w** = [wâ‚, wâ‚‚, ..., wâ‚™]áµ€:

```
v + w = [vâ‚ + wâ‚]
        [vâ‚‚ + wâ‚‚]
        [  â‹®   ]
        [vâ‚™ + wâ‚™]
```

**Properties:**
- **Commutative:** v + w = w + v
- **Associative:** (u + v) + w = u + (v + w)
- **Identity:** v + 0 = v

#### Scalar Multiplication
For scalar c and vector **v**:

```
cv = [cvâ‚]
     [cvâ‚‚]
     [ â‹® ]
     [cvâ‚™]
```

**Properties:**
- **Associative:** (ab)v = a(bv)
- **Distributive:** a(v + w) = av + aw
- **Distributive:** (a + b)v = av + bv

#### Dot Product (Inner Product)
For vectors **v** and **w**:

```
v Â· w = vâ‚wâ‚ + vâ‚‚wâ‚‚ + ... + vâ‚™wâ‚™ = Î£áµ¢â‚Œâ‚â¿ váµ¢wáµ¢
```

**Geometric interpretation:**
```
v Â· w = |v||w|cos(Î¸)
```
where Î¸ is the angle between vectors.

**Properties:**
- **Commutative:** v Â· w = w Â· v
- **Distributive:** u Â· (v + w) = u Â· v + u Â· w
- **Scalar multiplication:** (cv) Â· w = c(v Â· w)

#### Vector Magnitude (Norm)
The **Euclidean norm** (or magnitude) of vector **v**:

```
|v| = âˆš(v Â· v) = âˆš(vâ‚Â² + vâ‚‚Â² + ... + vâ‚™Â²)
```

#### Unit Vector
A **unit vector** in the direction of **v**:

```
vÌ‚ = v/|v|
```

**Properties:**
- |vÌ‚| = 1
- vÌ‚ points in the same direction as v

### Applications
- **Physics:** Force, velocity, acceleration vectors
- **Computer Graphics:** 3D transformations, lighting calculations
- **Machine Learning:** Feature vectors, similarity measures

---

## 2ï¸âƒ£ Linear Combinations, Span, and Basis

### Linear Combinations

A **linear combination** of vectors $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k$ is any vector of the form:
$$\mathbf{w} = c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \cdots + c_k\mathbf{v}_k$$
where $c_1, c_2, \ldots, c_k$ are scalars.

**Example:**
If $\mathbf{v}_1 = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$ and $\mathbf{v}_2 = \begin{bmatrix} 3 \\ 1 \end{bmatrix}$, then:
$$2\mathbf{v}_1 + 3\mathbf{v}_2 = 2\begin{bmatrix} 1 \\ 2 \end{bmatrix} + 3\begin{bmatrix} 3 \\ 1 \end{bmatrix} = \begin{bmatrix} 2 \\ 4 \end{bmatrix} + \begin{bmatrix} 9 \\ 3 \end{bmatrix} = \begin{bmatrix} 11 \\ 7 \end{bmatrix}$$

### Linear Independence

Vectors $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k$ are **linearly independent** if:
$$c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \cdots + c_k\mathbf{v}_k = \mathbf{0}$$
only when $c_1 = c_2 = \cdots = c_k = 0$.

**Test for linear independence:**
- For 2D vectors: $\mathbf{v}_1$ and $\mathbf{v}_2$ are independent if $\mathbf{v}_1 \neq k\mathbf{v}_2$ for any scalar $k$
- For 3D vectors: $\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3$ are independent if $\det([\mathbf{v}_1 \mathbf{v}_2 \mathbf{v}_3]) \neq 0$

### Span

The **span** of vectors $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k$ is the set of all possible linear combinations:
$$\text{Span}\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k\} = \{c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \cdots + c_k\mathbf{v}_k : c_i \in \mathbb{R}\}$$

**Geometric interpretation:** 
- Span of one vector: a line through the origin
- Span of two independent vectors: a plane through the origin
- Span of three independent vectors in $\mathbb{R}^3$: all of $\mathbb{R}^3$

### Basis

A **basis** for a vector space $V$ is a set of vectors that:
1. **Spans V:** Every vector in $V$ can be written as a linear combination of basis vectors
2. **Is linearly independent:** No basis vector can be written as a linear combination of the others

**Standard basis for $\mathbb{R}^n$:**
$$\mathbf{e}_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}, \quad \mathbf{e}_2 = \begin{bmatrix} 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}, \quad \ldots, \quad \mathbf{e}_n = \begin{bmatrix} 0 \\ 0 \\ 0 \\ \vdots \\ 1 \end{bmatrix}$$

### Dimension

The **dimension** of a vector space is the number of vectors in any basis for that space.

**Properties:**
- $\dim(\mathbb{R}^n) = n$
- $\dim(\text{Span}\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k\}) =$ number of linearly independent vectors in $\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k\}$

### Examples

**Example 1:** In $\mathbb{R}^2$
- $\mathbf{v}_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$, $\mathbf{v}_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$ form a basis
- Any vector $\begin{bmatrix} a \\ b \end{bmatrix} = a\begin{bmatrix} 1 \\ 0 \end{bmatrix} + b\begin{bmatrix} 0 \\ 1 \end{bmatrix}$

**Example 2:** In $\mathbb{R}^3$
- $\mathbf{v}_1 = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}$, $\mathbf{v}_2 = \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix}$ span a plane
- Adding $\mathbf{v}_3 = \begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}$ gives a basis for $\mathbb{R}^3$

### Applications
- **Computer Graphics:** Coordinate systems, transformations
- **Machine Learning:** Feature spaces, dimensionality reduction
- **Physics:** Coordinate transformations, quantum mechanics

---

## 3ï¸âƒ£ Linear Transformations and Matrices

### Linear Transformations

A **linear transformation** $T: \mathbb{R}^n \to \mathbb{R}^m$ is a function that maps vectors from one space to another while preserving vector addition and scalar multiplication.

**Definition:**
$$T(a\mathbf{u} + b\mathbf{v}) = aT(\mathbf{u}) + bT(\mathbf{v})$$
for all vectors $\mathbf{u}, \mathbf{v}$ and scalars $a, b$.

**Properties:**
- $T(\mathbf{0}) = \mathbf{0}$ (maps zero vector to zero vector)
- $T(-\mathbf{v}) = -T(\mathbf{v})$ (preserves negation)

### Matrix Representation

Every linear transformation $T: \mathbb{R}^n \to \mathbb{R}^m$ can be represented by an $m \times n$ matrix $A$:
$$T(\mathbf{x}) = A\mathbf{x}$$

**How to find the matrix:**
1. Apply $T$ to each standard basis vector $\mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_n$
2. The columns of $A$ are $T(\mathbf{e}_1), T(\mathbf{e}_2), \ldots, T(\mathbf{e}_n)$

### Types of Matrices

#### Square Matrices ($n \times n$)
- **Identity matrix:** $I = \text{diag}(1, 1, \ldots, 1)$
- **Diagonal matrix:** $D = \text{diag}(d_1, d_2, \ldots, d_n)$
- **Symmetric matrix:** $A = A^T$
- **Skew-symmetric matrix:** $A = -A^T$
- **Orthogonal matrix:** $A^TA = I$ (preserves lengths and angles)

#### Special Matrices
- **Zero matrix:** $O$ (all entries are 0)
- **Upper triangular:** entries below main diagonal are 0
- **Lower triangular:** entries above main diagonal are 0
- **Permutation matrix:** exactly one 1 in each row/column

### Matrix Operations

#### Addition
For matrices $A$ and $B$ of the same size:
$$(A + B)_{ij} = A_{ij} + B_{ij}$$

**Properties:**
- **Commutative:** $A + B = B + A$
- **Associative:** $(A + B) + C = A + (B + C)$

#### Scalar Multiplication
For scalar $c$ and matrix $A$:
$$(cA)_{ij} = c \cdot A_{ij}$$

#### Matrix Multiplication
For $A$ ($m \times n$) and $B$ ($n \times p$):
$$(AB)_{ij} = \sum_{k=1}^{n} A_{ik}B_{kj}$$

**Properties:**
- **Associative:** $(AB)C = A(BC)$
- **Distributive:** $A(B + C) = AB + AC$
- **NOT commutative:** $AB \neq BA$ in general

#### Transpose
$$(A^T)_{ij} = A_{ji}$$

**Properties:**
- $(A^T)^T = A$
- $(A + B)^T = A^T + B^T$
- $(AB)^T = B^TA^T$

### Examples

**Example 1: Rotation Matrix (2D)**
$$R(\theta) = \begin{bmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{bmatrix}$$

**Example 2: Scaling Matrix**
$$S = \begin{bmatrix} s_x & 0 \\ 0 & s_y \end{bmatrix}$$

**Example 3: Reflection Matrix (across x-axis)**
$$F = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}$$

### Applications
- **Computer Graphics:** 3D transformations, rotations, scaling
- **Physics:** Coordinate transformations, quantum mechanics
- **Engineering:** Structural analysis, signal processing
- **Machine Learning:** Feature transformations, dimensionality reduction

---

## 4ï¸âƒ£ Matrix Multiplication as Composition

Matrix multiplication represents **composition of linear transformations**.

### Composition of Transformations

If Tâ‚: â„â¿ â†’ â„áµ and Tâ‚‚: â„áµ â†’ â„áµ– are linear transformations with matrices A and B respectively, then:
```
Tâ‚‚(Tâ‚(x)) = Tâ‚‚(Ax) = B(Ax) = (BA)x
```

**Key insight:** The matrix BA represents doing Tâ‚ first, then Tâ‚‚.

### Order Matters

**Important:** Matrix multiplication is not commutative!
- AB represents: "do B first, then A"
- BA represents: "do A first, then B"

### Examples

**Example 1: Rotation followed by Scaling**
```
R = [cos(Î¸) -sin(Î¸)]  (rotation)
    [sin(Î¸)  cos(Î¸)]

S = [2  0]            (scaling)
    [0  3]

SR = [2cos(Î¸) -2sin(Î¸)]  (scale then rotate)
     [3sin(Î¸)  3cos(Î¸)]

RS = [2cos(Î¸) -3sin(Î¸)]  (rotate then scale)
     [2sin(Î¸)  3cos(Î¸)]
```

**Example 2: Multiple Transformations**
For transformations Tâ‚, Tâ‚‚, Tâ‚ƒ with matrices Aâ‚, Aâ‚‚, Aâ‚ƒ:
```
Tâ‚ƒ(Tâ‚‚(Tâ‚(x))) = Aâ‚ƒ(Aâ‚‚(Aâ‚x)) = (Aâ‚ƒAâ‚‚Aâ‚)x
```

### Properties of Composition

1. **Associative:** (AB)C = A(BC)
2. **Identity:** AI = IA = A
3. **Distributive:** A(B + C) = AB + AC

### Applications
- **Computer Graphics:** Combining multiple transformations
- **Robotics:** Kinematic chains, coordinate transformations
- **Physics:** Successive coordinate system changes

---

## 5ï¸âƒ£ Three-Dimensional Linear Transformations

In 3D space, linear transformations can be represented by 3Ã—3 matrices acting on vectors [x, y, z]áµ€.

### Common 3D Transformations

#### Rotation Matrices

**Rotation around x-axis:**
```
Râ‚“(Î¸) = [1     0        0   ]
        [0  cos(Î¸) -sin(Î¸)]
        [0  sin(Î¸)  cos(Î¸)]
```

**Rotation around y-axis:**
```
Ráµ§(Î¸) = [ cos(Î¸)  0  sin(Î¸)]
        [   0     1    0   ]
        [-sin(Î¸)  0  cos(Î¸)]
```

**Rotation around z-axis:**
```
Ráµ§(Î¸) = [cos(Î¸) -sin(Î¸)  0]
        [sin(Î¸)  cos(Î¸)  0]
        [  0       0     1]
```

#### Scaling Matrix
```
S = [sâ‚“  0   0 ]
    [ 0  sáµ§  0 ]
    [ 0   0  sáµ§]
```

#### Reflection Matrices

**Reflection across xy-plane:**
```
Fâ‚“áµ§ = [1  0  0]
      [0  1  0]
      [0  0 -1]
```

#### Shear Matrices

**Shear in x-direction:**
```
Hâ‚“ = [1  hâ‚“áµ§  hâ‚“áµ§]
     [0   1    0 ]
     [0   0    1 ]
```

### Composition of 3D Transformations

**Example: Rotation around arbitrary axis**
To rotate around axis u = [uâ‚“, uáµ§, uáµ§] by angle Î¸:
```
R = cos(Î¸)I + sin(Î¸)[u]Ã— + (1-cos(Î¸))(u âŠ— u)
```
where [u]Ã— is the skew-symmetric matrix and u âŠ— u is the outer product.

### Applications
- **Computer Graphics:** 3D modeling, animation, rendering
- **Robotics:** Robot arm kinematics, coordinate transformations
- **Physics:** Rigid body dynamics, celestial mechanics
- **Engineering:** Structural analysis, CAD systems

---

## 6ï¸âƒ£ The Determinant

The **determinant** is a scalar value that provides important information about a square matrix and its associated linear transformation.

### Definition
For an $n \times n$ matrix $A$, the determinant is defined recursively:
$$\det(A) = \sum_{j=1}^{n} (-1)^{i+j} a_{ij} \det(A_{ij})$$
where $A_{ij}$ is the $(n-1) \times (n-1)$ matrix obtained by deleting row $i$ and column $j$.

### Geometric Interpretation

The determinant measures how a linear transformation scales area (2D) or volume (3D):
- **2D:** $\det(A) =$ area scaling factor
- **3D:** $\det(A) =$ volume scaling factor
- **nD:** $\det(A) =$ $n$-dimensional volume scaling factor

### Properties of Determinants

1. **Zero determinant:** $\det(A) = 0 \iff A$ is singular (not invertible)
2. **Sign:** 
   - $\det(A) > 0 \iff$ orientation preserved
   - $\det(A) < 0 \iff$ orientation reversed
3. **Multiplicative:** $\det(AB) = \det(A)\det(B)$
4. **Transpose:** $\det(A^T) = \det(A)$
5. **Scalar multiplication:** $\det(cA) = c^n\det(A)$ for $n \times n$ matrix $A$

### Computing Determinants

#### 2Ã—2 Matrix
$$\det\begin{pmatrix} a & b \\ c & d \end{pmatrix} = ad - bc$$

#### 3Ã—3 Matrix (Sarrus' Rule)
$$\det\begin{pmatrix} a & b & c \\ d & e & f \\ g & h & i \end{pmatrix} = a(ei - fh) - b(di - fg) + c(dh - eg)$$

#### General nÃ—n Matrix (Laplace Expansion)
$$\det(A) = \sum_{j=1}^{n} (-1)^{i+j} a_{ij} \det(A_{ij})$$
where $A_{ij}$ is the matrix obtained by deleting row $i$ and column $j$.

### Cofactor Expansion

For any row $i$:
$$\det(A) = \sum_{j=1}^{n} a_{ij} C_{ij}$$
where $C_{ij} = (-1)^{i+j} \det(A_{ij})$ is the cofactor.

### Examples

**Example 1: 2Ã—2 Matrix**
$$A = \begin{pmatrix} 3 & 1 \\ 2 & 4 \end{pmatrix}$$
$$\det(A) = 3 \cdot 4 - 1 \cdot 2 = 12 - 2 = 10$$

**Example 2: 3Ã—3 Matrix**
$$A = \begin{pmatrix} 1 & 2 & 3 \\ 0 & 4 & 5 \\ 0 & 0 & 6 \end{pmatrix}$$
$$\det(A) = 1 \cdot 4 \cdot 6 = 24 \quad \text{(triangular matrix)}$$

### Applications

1. **Invertibility:** A is invertible âŸº det(A) â‰  0
2. **Volume/Area:** Computing volumes of parallelepipeds
3. **Change of variables:** Jacobian determinant in calculus
4. **Eigenvalues:** Characteristic polynomial roots
5. **Cross product:** |v Ã— w| = |v||w|sin(Î¸) = det([v w])

---

## 7ï¸âƒ£ Inverse Matrices, Column Space, and Null Space

### Inverse Matrices

The **inverse** of a square matrix A is a matrix Aâ»Â¹ such that:
```
AÂ·Aâ»Â¹ = Aâ»Â¹Â·A = I
```

#### Conditions for Invertibility
A matrix A is invertible if and only if:
- det(A) â‰  0
- A has full rank
- The columns of A are linearly independent
- A is nonsingular

#### Computing the Inverse

**For 2Ã—2 matrices:**
```
Aâ»Â¹ = (1/det(A)) [ d -b]
                 [-c  a]
```

**For general matrices (Gauss-Jordan elimination):**
1. Form the augmented matrix [A | I]
2. Apply row operations to get [I | Aâ»Â¹]

#### Properties of Inverses
- (Aâ»Â¹)â»Â¹ = A
- (AB)â»Â¹ = Bâ»Â¹Aâ»Â¹
- (Aáµ€)â»Â¹ = (Aâ»Â¹)áµ€
- det(Aâ»Â¹) = 1/det(A)

### Column Space (Range)

The **column space** Col(A) of matrix A is the span of its columns:
```
Col(A) = Span{aâ‚, aâ‚‚, ..., aâ‚™}
```
where aáµ¢ is the i-th column of A.

**Properties:**
- Col(A) = {Ax : x âˆˆ â„â¿} (all possible outputs)
- dim(Col(A)) = rank(A)
- Col(A) âŠ† â„áµ (where A is mÃ—n)

### Null Space (Kernel)

The **null space** Null(A) of matrix A is:
```
Null(A) = {x âˆˆ â„â¿ : Ax = 0}
```

**Properties:**
- Null(A) is a subspace of â„â¿
- dim(Null(A)) = n - rank(A)
- Null(A) = {0} âŸº A has full column rank

### Rank-Nullity Theorem

For an mÃ—n matrix A:
```
rank(A) + nullity(A) = n
```
where nullity(A) = dim(Null(A)).

### Examples

**Example 1: Finding Column Space**
```
A = [1 2]
    [3 4]
Col(A) = Span{[1,3], [2,4]} = â„Â²
```

**Example 2: Finding Null Space**
```
A = [1 2 3]
    [0 1 2]
Null(A) = {x : xâ‚ + 2xâ‚‚ + 3xâ‚ƒ = 0, xâ‚‚ + 2xâ‚ƒ = 0}
        = Span{[1, -2, 1]}
```

### Applications
- **Solving linear systems:** Ax = b has solution âŸº b âˆˆ Col(A)
- **Linear independence:** Vectors are independent âŸº null space is {0}
- **Dimensionality:** Understanding the "size" of solution spaces
- **Machine Learning:** Feature spaces, principal component analysis

---

## 8ï¸âƒ£ Nonsquare Matrices as Transformations Between Dimensions

An mÃ—n matrix A represents a linear transformation from â„â¿ to â„áµ.

### Dimension Changes

#### Embedding (m > n)
When m > n, the transformation **embeds** a lower-dimensional space into a higher-dimensional one.

**Example:**
```
A = [1 0]    (2Ã—1 matrix)
    [0 1]
    [1 1]
```
Maps â„Â¹ â†’ â„Â³: x â†¦ [x, 0, x]áµ€

#### Projection (m < n)
When m < n, the transformation **projects** a higher-dimensional space onto a lower-dimensional one.

**Example:**
```
A = [1 0 0]  (1Ã—3 matrix)
```
Maps â„Â³ â†’ â„Â¹: [x, y, z]áµ€ â†¦ x

### Rank and Dimension

For an mÃ—n matrix A:
- **Column rank:** dim(Col(A)) â‰¤ min(m, n)
- **Row rank:** dim(Row(A)) â‰¤ min(m, n)
- **Rank:** rank(A) = column rank = row rank

### Examples

**Example 1: 3Ã—2 Matrix (Embedding)**
```
A = [1 0]
    [0 1]
    [1 1]
```
- Maps â„Â² â†’ â„Â³
- Col(A) = plane in â„Â³
- Null(A) = {0}

**Example 2: 2Ã—3 Matrix (Projection)**
```
A = [1 0 0]
    [0 1 0]
```
- Maps â„Â³ â†’ â„Â²
- Col(A) = â„Â²
- Null(A) = Span{[0, 0, 1]áµ€}

### Applications
- **Data compression:** Reducing dimensionality while preserving information
- **Computer graphics:** Projecting 3D objects onto 2D screens
- **Machine Learning:** Feature extraction, dimensionality reduction
- **Signal processing:** Sampling and reconstruction

---

## 9ï¸âƒ£ Dot Products and Duality

### Dot Product (Inner Product)

The **dot product** of vectors v and w is:
```
v Â· w = vâ‚wâ‚ + vâ‚‚wâ‚‚ + ... + vâ‚™wâ‚™ = Î£áµ¢â‚Œâ‚â¿ váµ¢wáµ¢
```

#### Geometric Interpretation
```
v Â· w = |v||w|cos(Î¸)
```
where Î¸ is the angle between the vectors.

#### Properties
- **Commutative:** v Â· w = w Â· v
- **Distributive:** u Â· (v + w) = u Â· v + u Â· w
- **Scalar multiplication:** (cv) Â· w = c(v Â· w)
- **Positive definite:** v Â· v â‰¥ 0, with equality only if v = 0

### Projection

The **projection** of vector v onto vector w is:
```
proj_w(v) = (v Â· w / |w|Â²)w = (v Â· Åµ)Åµ
```
where Åµ = w/|w| is the unit vector in the direction of w.

### Orthogonality

Vectors v and w are **orthogonal** if:
```
v Â· w = 0
```

### Duality

The **duality principle** states that every vector v can be viewed as:
1. **A geometric object:** a directed line segment
2. **A linear functional:** a function that maps other vectors to scalars

#### Vector as Linear Functional

For any vector v, we can define a linear functional:
```
f_v(w) = v Â· w
```

This functional:
- Maps â„â¿ â†’ â„
- Is linear: f_v(aw + bu) = af_v(w) + bf_v(u)
- Is bounded: |f_v(w)| â‰¤ |v||w|

#### Matrix Representation

The linear functional f_v can be represented as a row vector:
```
f_v(w) = váµ€w
```

### Examples

**Example 1: Computing Dot Product**
```
v = [3, 4], w = [1, 2]
v Â· w = 3Â·1 + 4Â·2 = 3 + 8 = 11
```

**Example 2: Projection**
```
v = [3, 4], w = [1, 0]
proj_w(v) = (3Â·1 + 4Â·0)/1Â² Â· [1, 0] = 3[1, 0] = [3, 0]
```

**Example 3: Orthogonality Check**
```
v = [1, -1], w = [1, 1]
v Â· w = 1Â·1 + (-1)Â·1 = 1 - 1 = 0 âœ“ (orthogonal)
```

### Applications
- **Physics:** Work done by force, energy calculations
- **Computer Graphics:** Lighting models, shading
- **Machine Learning:** Similarity measures, feature selection
- **Signal Processing:** Correlation, filtering
- **Optimization:** Gradient descent, least squares

---

## ğŸ”Ÿ Cross Product

The **cross product** is a binary operation on vectors in $\mathbb{R}^3$ that produces a vector perpendicular to both input vectors.

### Definition

For vectors $\mathbf{v} = [v_1, v_2, v_3]^T$ and $\mathbf{w} = [w_1, w_2, w_3]^T$ in $\mathbb{R}^3$:
$$\mathbf{v} \times \mathbf{w} = \begin{bmatrix} v_2w_3 - v_3w_2 \\ v_3w_1 - v_1w_3 \\ v_1w_2 - v_2w_1 \end{bmatrix}$$

### Properties

#### Geometric Properties
- **Direction:** $\mathbf{v} \times \mathbf{w}$ is perpendicular to both $\mathbf{v}$ and $\mathbf{w}$
- **Magnitude:** $|\mathbf{v} \times \mathbf{w}| = |\mathbf{v}||\mathbf{w}|\sin(\theta)$
- **Right-hand rule:** Points in direction determined by right-hand rule

#### Algebraic Properties
- **Anticommutative:** $\mathbf{v} \times \mathbf{w} = -(\mathbf{w} \times \mathbf{v})$
- **Distributive:** $\mathbf{u} \times (\mathbf{v} + \mathbf{w}) = \mathbf{u} \times \mathbf{v} + \mathbf{u} \times \mathbf{w}$
- **Scalar multiplication:** $(c\mathbf{v}) \times \mathbf{w} = c(\mathbf{v} \times \mathbf{w}) = \mathbf{v} \times (c\mathbf{w})$
- **Self-product:** $\mathbf{v} \times \mathbf{v} = \mathbf{0}$

### Determinant Formula

The cross product can be computed using a determinant:
$$\mathbf{v} \times \mathbf{w} = \det\begin{pmatrix} \mathbf{i} & \mathbf{j} & \mathbf{k} \\ v_1 & v_2 & v_3 \\ w_1 & w_2 & w_3 \end{pmatrix}$$

### Examples

**Example 1: Basic Cross Product**
$$\mathbf{v} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \quad \mathbf{w} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}$$
$$\mathbf{v} \times \mathbf{w} = \begin{bmatrix} 0 \cdot 0 - 0 \cdot 1 \\ 0 \cdot 0 - 1 \cdot 0 \\ 1 \cdot 1 - 0 \cdot 0 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$$

**Example 2: Using Determinant**
$$\mathbf{v} = \begin{bmatrix} 2 \\ 3 \\ 4 \end{bmatrix}, \quad \mathbf{w} = \begin{bmatrix} 5 \\ 6 \\ 7 \end{bmatrix}$$
$$\mathbf{v} \times \mathbf{w} = \det\begin{pmatrix} \mathbf{i} & \mathbf{j} & \mathbf{k} \\ 2 & 3 & 4 \\ 5 & 6 & 7 \end{pmatrix}$$
$$= \mathbf{i}(3 \cdot 7 - 4 \cdot 6) - \mathbf{j}(2 \cdot 7 - 4 \cdot 5) + \mathbf{k}(2 \cdot 6 - 3 \cdot 5)$$
$$= \mathbf{i}(21 - 24) - \mathbf{j}(14 - 20) + \mathbf{k}(12 - 15) = \begin{bmatrix} -3 \\ 6 \\ -3 \end{bmatrix}$$

### Applications

#### Area and Volume
- **Parallelogram area:** $|\mathbf{v} \times \mathbf{w}|$
- **Triangle area:** $\frac{1}{2}|\mathbf{v} \times \mathbf{w}|$
- **Parallelepiped volume:** $|\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w})|$

#### Physics
- **Torque:** $\boldsymbol{\tau} = \mathbf{r} \times \mathbf{F}$
- **Angular momentum:** $\mathbf{L} = \mathbf{r} \times \mathbf{p}$
- **Magnetic force:** $\mathbf{F} = q(\mathbf{v} \times \mathbf{B})$

#### Computer Graphics
- **Surface normals:** Computing perpendicular vectors
- **Rotation axes:** Finding rotation directions
- **Orientation:** Determining handedness of coordinate systems

### Triple Products

#### Scalar Triple Product
$$\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w}) = \det\begin{pmatrix} u_1 & u_2 & u_3 \\ v_1 & v_2 & v_3 \\ w_1 & w_2 & w_3 \end{pmatrix}$$

#### Vector Triple Product
$$\mathbf{u} \times (\mathbf{v} \times \mathbf{w}) = \mathbf{v}(\mathbf{u} \cdot \mathbf{w}) - \mathbf{w}(\mathbf{u} \cdot \mathbf{v})$$

---

## 11ï¸âƒ£ Cross Product as Linear Transformation

The cross product can be expressed as a **matrix multiplication**, revealing its linear nature.

### Skew-Symmetric Matrix Representation

For any vector v = [vâ‚, vâ‚‚, vâ‚ƒ], we can define the **skew-symmetric matrix**:
```
[v]Ã— = [ 0  -vâ‚ƒ  vâ‚‚]
       [ vâ‚ƒ  0  -vâ‚]
       [-vâ‚‚  vâ‚  0 ]
```

### Cross Product as Matrix Multiplication

The cross product v Ã— w can be written as:
```
v Ã— w = [v]Ã— Â· w
```

### Properties of Skew-Symmetric Matrices

#### General Properties
- **[v]Ã—áµ€ = -[v]Ã—** (skew-symmetric)
- **[v]Ã— Â· v = 0** (any vector crossed with itself is zero)
- **[v]Ã— Â· w = -[w]Ã— Â· v** (anticommutativity)

#### Eigenvalues and Eigenvectors
For [v]Ã— where v â‰  0:
- **Eigenvalue 0:** with eigenvector v (since [v]Ã— Â· v = 0)
- **Eigenvalues Â±i|v|:** with complex eigenvectors

### Examples

**Example 1: Basic Skew-Symmetric Matrix**
```
v = [1, 2, 3]
[v]Ã— = [ 0  -3   2]
       [ 3   0  -1]
       [-2   1   0]
```

**Example 2: Cross Product via Matrix**
```
v = [1, 0, 0], w = [0, 1, 0]
[v]Ã— = [ 0  0  0]
       [ 0  0 -1]
       [ 0  1  0]

v Ã— w = [v]Ã— Â· w = [ 0  0  0] [0]   [0]
                        [ 0  0 -1] [1] = [0]
                        [ 0  1  0] [0]   [1]
```

### Applications

#### Rotation Matrices
The skew-symmetric matrix is fundamental in rotation theory:
```
R = I + sin(Î¸)[Ã»]Ã— + (1-cos(Î¸))[Ã»]Ã—Â²
```
where Ã» is a unit vector and Î¸ is the rotation angle.

#### Lie Algebras
Skew-symmetric matrices form the Lie algebra so(3) of the rotation group SO(3).

#### Robotics
Used in:
- **Kinematics:** Representing angular velocities
- **Dynamics:** Moment of inertia tensors
- **Control:** Feedback linearization

### Connection to Quaternions

The skew-symmetric matrix representation connects to quaternion multiplication:
```
qâ‚qâ‚‚ = (sâ‚ + vâ‚)(sâ‚‚ + vâ‚‚) = sâ‚sâ‚‚ - vâ‚Â·vâ‚‚ + sâ‚vâ‚‚ + sâ‚‚vâ‚ + vâ‚Ã—vâ‚‚
```
where the cross product term corresponds to the skew-symmetric part.

---

## 12ï¸âƒ£ Cramer's Rule (Geometric View)

Cramer's Rule provides a geometric method for solving linear systems using determinants.

### The Rule

For the system Ax = b where A is an nÃ—n invertible matrix:
```
xáµ¢ = det(Aáµ¢) / det(A)
```
where Aáµ¢ is the matrix obtained by replacing the i-th column of A with the vector b.

### Geometric Interpretation

Each solution component xáµ¢ represents the **ratio of volumes** (or areas in 2D):
- **Numerator:** Volume of parallelepiped formed by replacing column i with b
- **Denominator:** Volume of parallelepiped formed by original columns of A

### Step-by-Step Process

1. **Check invertibility:** det(A) â‰  0
2. **For each variable xáµ¢:**
   - Replace column i of A with b to get Aáµ¢
   - Compute det(Aáµ¢)
   - Calculate xáµ¢ = det(Aáµ¢) / det(A)

### Examples

**Example 1: 2Ã—2 System**
```
2x + 3y = 7
4x + 5y = 11

A = [2 3], b = [7]
    [4 5]     [11]

det(A) = 2Â·5 - 3Â·4 = 10 - 12 = -2

Aâ‚ = [7 3], det(Aâ‚) = 7Â·5 - 3Â·11 = 35 - 33 = 2
     [11 5]

Aâ‚‚ = [2 7], det(Aâ‚‚) = 2Â·11 - 7Â·4 = 22 - 28 = -6
     [4 11]

x = det(Aâ‚)/det(A) = 2/(-2) = -1
y = det(Aâ‚‚)/det(A) = (-6)/(-2) = 3
```

**Example 2: 3Ã—3 System**
```
x + 2y + 3z = 6
2x + 5y + 2z = 4
6x - 3y + z = 2

A = [1  2  3], b = [6]
    [2  5  2]     [4]
    [6 -3  1]     [2]

det(A) = 1(5Â·1 - 2Â·(-3)) - 2(2Â·1 - 2Â·6) + 3(2Â·(-3) - 5Â·6)
       = 1(5 + 6) - 2(2 - 12) + 3(-6 - 30)
       = 11 - 2(-10) + 3(-36)
       = 11 + 20 - 108 = -77

x = det(Aâ‚)/det(A), y = det(Aâ‚‚)/det(A), z = det(Aâ‚ƒ)/det(A)
```

### Advantages and Disadvantages

#### Advantages
- **Geometric insight:** Direct connection to volumes/areas
- **Explicit formulas:** No iterative methods needed
- **Theoretical importance:** Shows relationship between determinants and solutions

#### Disadvantages
- **Computational cost:** O(n!) for nÃ—n systems
- **Numerical instability:** For large matrices or near-singular systems
- **Limited applicability:** Only works for square, invertible systems

### Applications
- **Small systems:** 2Ã—2 and 3Ã—3 systems where geometric insight is valuable
- **Theoretical analysis:** Understanding the relationship between determinants and solutions
- **Educational purposes:** Demonstrating geometric principles in linear algebra

---

## 13ï¸âƒ£ Change of Basis

Changing basis means expressing the same vector in different coordinate systems.

### Basis Change Matrix

If B = {vâ‚, vâ‚‚, ..., vâ‚™} is a new basis for â„â¿, the **change of basis matrix** is:
```
P = [vâ‚ vâ‚‚ ... vâ‚™]
```

### Coordinate Transformations

#### From New Basis to Standard Basis
```
x = P Â· x_B
```
where x_B are coordinates in the new basis.

#### From Standard Basis to New Basis
```
x_B = Pâ»Â¹ Â· x
```

### Properties

1. **P is invertible** âŸº {vâ‚, vâ‚‚, ..., vâ‚™} is a basis
2. **Pâ»Â¹ exists** and represents the inverse transformation
3. **Composition:** If Pâ‚: Bâ‚ â†’ Bâ‚‚ and Pâ‚‚: Bâ‚‚ â†’ Bâ‚ƒ, then Pâ‚‚Pâ‚: Bâ‚ â†’ Bâ‚ƒ

### Examples

**Example 1: Simple Basis Change**
```
Standard basis: eâ‚ = [1,0], eâ‚‚ = [0,1]
New basis: vâ‚ = [1,1], vâ‚‚ = [1,-1]

P = [1  1]
    [1 -1]

Pâ»Â¹ = (1/2)[1  1]
           [1 -1]

Vector [3,4] in standard basis:
[3,4]_B = Pâ»Â¹[3,4] = (1/2)[1  1][3] = (1/2)[7] = [3.5]
                          [1 -1][4]        [-1]   [-0.5]
```

**Example 2: Rotation Basis**
```
Standard basis: eâ‚ = [1,0], eâ‚‚ = [0,1]
Rotated basis: vâ‚ = [cos(Î¸),sin(Î¸)], vâ‚‚ = [-sin(Î¸),cos(Î¸)]

P = [cos(Î¸) -sin(Î¸)]
    [sin(Î¸)  cos(Î¸)]

Pâ»Â¹ = Páµ€ (orthogonal matrix)
```

### Matrix Representation of Linear Transformations

If T has matrix A in standard basis, then in new basis B:
```
A_B = Pâ»Â¹AP
```

### Applications

#### Diagonalization
If A has eigenvectors as columns of P, then:
```
Pâ»Â¹AP = D (diagonal matrix)
```

#### Principal Component Analysis
- **Data transformation:** Rotating to principal axes
- **Dimensionality reduction:** Projecting onto important directions

#### Computer Graphics
- **Coordinate systems:** World, camera, screen coordinates
- **Transformations:** Rotating, scaling, translating objects

#### Quantum Mechanics
- **Representation theory:** Different bases for quantum states
- **Unitary transformations:** Preserving probabilities

### Orthogonal Change of Basis

When P is orthogonal (Páµ€P = I):
- **Preserves lengths:** |Px| = |x|
- **Preserves angles:** (Px)Â·(Py) = xÂ·y
- **Inverse is transpose:** Pâ»Â¹ = Páµ€

---

## 14ï¸âƒ£ Eigenvectors and Eigenvalues

Eigenvectors and eigenvalues are fundamental concepts that reveal the "natural directions" of linear transformations.

### Definition

For a square matrix $A$, a nonzero vector $\mathbf{v}$ is an **eigenvector** with **eigenvalue** $\lambda$ if:
$$A\mathbf{v} = \lambda\mathbf{v}$$

### Characteristic Polynomial

The eigenvalues are roots of the **characteristic polynomial**:
$$p(\lambda) = \det(A - \lambda I) = 0$$

### Computing Eigenvalues

#### 2Ã—2 Matrix
For $A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$, the characteristic polynomial is:
$$\lambda^2 - (a + d)\lambda + (ad - bc) = 0$$

Using the quadratic formula:
$$\lambda = \frac{(a + d) \pm \sqrt{(a + d)^2 - 4(ad - bc)}}{2}$$

#### 3Ã—3 Matrix
For $A = \begin{pmatrix} a & b & c \\ d & e & f \\ g & h & i \end{pmatrix}$, expand $\det(A - \lambda I)$:
$$\det(A - \lambda I) = (a - \lambda)[(e - \lambda)(i - \lambda) - fh] - b[d(i - \lambda) - fg] + c[dh - (e - \lambda)g]$$

### Computing Eigenvectors

For each eigenvalue $\lambda_i$, solve:
$$(A - \lambda_i I)\mathbf{v} = \mathbf{0}$$

This gives a homogeneous system whose solution space is the **eigenspace** $E(\lambda_i)$.

### Examples

**Example 1: 2Ã—2 Matrix**
$$A = \begin{pmatrix} 3 & 1 \\ 0 & 2 \end{pmatrix}$$

Characteristic polynomial: $\det\begin{pmatrix} 3-\lambda & 1 \\ 0 & 2-\lambda \end{pmatrix} = (3-\lambda)(2-\lambda) - 0 = 0$

$(3-\lambda)(2-\lambda) = 0$

$\lambda_1 = 3, \lambda_2 = 2$

For $\lambda_1 = 3$:
$$(A - 3I)\mathbf{v} = \begin{pmatrix} 0 & 1 \\ 0 & -1 \end{pmatrix}\begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$$
$y = 0$, $x$ arbitrary
Eigenvector: $\mathbf{v}_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$

For $\lambda_2 = 2$:
$$(A - 2I)\mathbf{v} = \begin{pmatrix} 1 & 1 \\ 0 & 0 \end{pmatrix}\begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$$
$x + y = 0$
Eigenvector: $\mathbf{v}_2 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}$

**Example 2: Symmetric Matrix**
$$A = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$$

Characteristic polynomial: $(2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3 = 0$
$\lambda_1 = 3, \lambda_2 = 1$

For $\lambda_1 = 3$: $\mathbf{v}_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$
For $\lambda_2 = 1$: $\mathbf{v}_2 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}$

### Properties

#### General Properties
- **Sum of eigenvalues:** $\lambda_1 + \lambda_2 + \cdots + \lambda_n = \text{trace}(A)$
- **Product of eigenvalues:** $\lambda_1\lambda_2\cdots\lambda_n = \det(A)$
- **Eigenvalues of powers:** $A^k$ has eigenvalues $\lambda_1^k, \lambda_2^k, \ldots, \lambda_n^k$

#### Special Cases
- **Symmetric matrices:** All eigenvalues are real
- **Orthogonal matrices:** All eigenvalues have $|\lambda| = 1$
- **Triangular matrices:** Eigenvalues are diagonal entries

### Diagonalization

A matrix $A$ is **diagonalizable** if there exists an invertible matrix $P$ such that:
$$P^{-1}AP = D$$
where $D$ is diagonal with eigenvalues on the diagonal.

**Conditions for diagonalization:**
- $A$ has $n$ linearly independent eigenvectors
- $A$ has $n$ distinct eigenvalues (sufficient but not necessary)

### Applications

#### Principal Component Analysis (PCA)
- **Data analysis:** Finding principal directions of variation
- **Dimensionality reduction:** Projecting onto eigenvectors

#### Stability Analysis
- **Dynamical systems:** Eigenvalues determine system stability
- **Control theory:** Pole placement, system design

#### Quantum Mechanics
- **Energy levels:** Eigenvalues of Hamiltonian operator
- **Quantum states:** Eigenvectors represent stationary states

#### Computer Graphics
- **Principal axes:** Finding natural orientations of objects
- **Compression:** Reducing data dimensionality

### Numerical Methods

#### Power Method
For finding the largest eigenvalue:
```
vâ‚–â‚Šâ‚ = Avâ‚– / |Avâ‚–|
```

#### QR Algorithm
Iterative method for finding all eigenvalues:
```
Aâ‚€ = A
Aâ‚–â‚Šâ‚ = Râ‚–Qâ‚– (where Aâ‚– = Qâ‚–Râ‚–)
```

### Geometric Interpretation

- **Eigenvectors:** Directions that don't change under transformation
- **Eigenvalues:** Scaling factors along eigenvector directions
- **Eigenspace:** Subspace of all eigenvectors for a given eigenvalue

---

## 15ï¸âƒ£ Abstract Vector Spaces

An **abstract vector space** generalizes the concept of vectors to any set of elements that satisfy the vector space axioms.

### Vector Space Axioms

A vector space V over a field F (usually â„ or â„‚) is a set with two operations:
1. **Vector addition:** V Ã— V â†’ V
2. **Scalar multiplication:** F Ã— V â†’ V

**Axioms:**
1. **Closure:** u + v âˆˆ V, cv âˆˆ V
2. **Commutativity:** u + v = v + u
3. **Associativity:** (u + v) + w = u + (v + w)
4. **Identity:** âˆƒ0 âˆˆ V such that v + 0 = v
5. **Inverse:** âˆ€v âˆˆ V, âˆƒ(-v) âˆˆ V such that v + (-v) = 0
6. **Distributivity:** c(u + v) = cu + cv, (c + d)v = cv + dv
7. **Associativity:** (cd)v = c(dv)
8. **Identity:** 1v = v

### Examples of Vector Spaces

#### Function Spaces
**C[a,b]:** Continuous functions on [a,b]
- Addition: (f + g)(x) = f(x) + g(x)
- Scalar multiplication: (cf)(x) = cf(x)

**Pâ‚™:** Polynomials of degree â‰¤ n
- Basis: {1, x, xÂ², ..., xâ¿}
- Dimension: n + 1

#### Matrix Spaces
**Mâ‚˜â‚“â‚™:** mÃ—n matrices
- Addition: (A + B)áµ¢â±¼ = Aáµ¢â±¼ + Báµ¢â±¼
- Scalar multiplication: (cA)áµ¢â±¼ = cAáµ¢â±¼
- Dimension: mn

#### Sequence Spaces
**â„“Â²:** Square-summable sequences
- Elements: (aâ‚, aâ‚‚, aâ‚ƒ, ...) where Î£|aáµ¢|Â² < âˆ
- Inner product: âŸ¨a, bâŸ© = Î£aáµ¢bÌ„áµ¢

### Subspaces

A **subspace** W of V is a subset that is itself a vector space.

**Subspace Test:** W âŠ† V is a subspace if:
1. 0 âˆˆ W
2. u, v âˆˆ W âŸ¹ u + v âˆˆ W
3. u âˆˆ W, c âˆˆ F âŸ¹ cu âˆˆ W

### Examples of Subspaces

**Example 1: Polynomial Subspaces**
- Pâ‚‚ âŠ† Pâ‚ƒ âŠ† Pâ‚„ âŠ† ...
- Even polynomials âŠ† Pâ‚™

**Example 2: Matrix Subspaces**
- Symmetric matrices âŠ† Mâ‚™â‚“â‚™
- Upper triangular matrices âŠ† Mâ‚™â‚“â‚™

### Linear Independence and Basis

#### Linear Independence
Vectors vâ‚, vâ‚‚, ..., vâ‚– are **linearly independent** if:
```
câ‚vâ‚ + câ‚‚vâ‚‚ + ... + câ‚–vâ‚– = 0 âŸ¹ câ‚ = câ‚‚ = ... = câ‚– = 0
```

#### Basis
A **basis** for V is a linearly independent spanning set.

**Examples:**
- **Pâ‚‚:** {1, x, xÂ²}
- **Mâ‚‚â‚“â‚‚:** {[1 0], [0 1], [0 0], [0 0]}
              [0 0]  [0 0]  [1 0]  [0 1]

### Dimension

The **dimension** of V is the number of vectors in any basis.

**Examples:**
- dim(â„â¿) = n
- dim(Pâ‚™) = n + 1
- dim(Mâ‚˜â‚“â‚™) = mn
- dim(C[a,b]) = âˆ (infinite-dimensional)

### Linear Transformations

A **linear transformation** T: V â†’ W between vector spaces satisfies:
```
T(câ‚vâ‚ + câ‚‚vâ‚‚) = câ‚T(vâ‚) + câ‚‚T(vâ‚‚)
```

**Examples:**
- **Differentiation:** D: Pâ‚™ â†’ Pâ‚™â‚‹â‚
- **Integration:** I: C[a,b] â†’ â„
- **Matrix multiplication:** T_A: â„â¿ â†’ â„áµ

### Applications

#### Signal Processing
- **Fourier analysis:** Functions as vectors in infinite-dimensional spaces
- **Filtering:** Linear transformations on signal spaces

#### Quantum Mechanics
- **State spaces:** Quantum states as vectors in Hilbert spaces
- **Operators:** Observables as linear transformations

#### Machine Learning
- **Feature spaces:** High-dimensional vector representations
- **Kernel methods:** Implicit mapping to infinite-dimensional spaces

#### Differential Equations
- **Solution spaces:** Sets of solutions form vector spaces
- **Linear operators:** Differential operators as linear transformations

---

## ğŸ¯ Practice Problems

### Basic Vector Operations
1. **Compute the dot product:** $\mathbf{v} = [3, -2, 1]^T$, $\mathbf{w} = [1, 4, -3]^T$
2. **Find the cross product:** $\mathbf{v} = [2, 0, -1]^T$, $\mathbf{w} = [1, 3, 2]^T$
3. **Normalize the vector:** $\mathbf{v} = [4, 3, 0]^T$

### Linear Independence
4. **Check if vectors are independent:** $\mathbf{v}_1 = [1, 2, 3]^T$, $\mathbf{v}_2 = [2, 4, 6]^T$, $\mathbf{v}_3 = [1, 0, 1]^T$
5. **Find a basis for the span:** $\text{Span}\{[1, 1, 0]^T, [0, 1, 1]^T, [1, 0, 1]^T\}$

### Matrix Operations
6. **Compute the determinant:** $A = \begin{pmatrix} 3 & 1 & 2 \\ 0 & 4 & 1 \\ 2 & 0 & 3 \end{pmatrix}$
7. **Find the inverse:** $A = \begin{pmatrix} 2 & 1 \\ 3 & 2 \end{pmatrix}$

### Eigenvalues and Eigenvectors
8. **Find eigenvalues and eigenvectors:** $A = \begin{pmatrix} 4 & 1 \\ 2 & 3 \end{pmatrix}$
9. **Diagonalize the matrix:** $A = \begin{pmatrix} 1 & 2 \\ 0 & 3 \end{pmatrix}$

### Applications
10. **PCA Problem:** Given data points $(1,2)$, $(2,4)$, $(3,6)$, find the principal components.

---

## ğŸ“š Summary

This comprehensive guide covers all essential linear algebra concepts with detailed mathematical formulations, examples, and practical applications. Each topic connects directly to:

### Core Concepts
- **Vector Spaces**: Foundation for all linear algebra
- **Linear Transformations**: How matrices transform vectors
- **Eigenvalues/Eigenvectors**: Natural directions of transformations
- **Determinants**: Volume scaling and invertibility
- **Matrix Operations**: Computational foundations

### Key Applications
- **Machine Learning**: Feature spaces, dimensionality reduction, PCA
- **Computer Graphics**: 3D transformations, rotations, projections
- **Physics**: Quantum mechanics, coordinate transformations
- **Engineering**: Signal processing, structural analysis
- **Data Science**: Principal component analysis, linear regression

### Mathematical Tools
- **Matrix decompositions**: Eigenvalue decomposition, SVD
- **Linear systems**: Solving $A\mathbf{x} = \mathbf{b}$
- **Geometric interpretations**: Understanding transformations visually
- **Computational methods**: Numerical linear algebra

This mathematical foundation is crucial for understanding and implementing algorithms in artificial intelligence, machine learning, computer graphics, and scientific computing.

---

## ğŸ“š Additional Resources

### Books
- **"Linear Algebra Done Right"** by Sheldon Axler
- **"Introduction to Linear Algebra"** by Gilbert Strang
- **"Linear Algebra and Its Applications"** by David Lay

### Online Resources
- **3Blue1Brown Linear Algebra Series** (YouTube)
- **Khan Academy Linear Algebra**
- **MIT OpenCourseWare 18.06**

### Software Tools
- **Python:** NumPy, SciPy, SymPy
- **MATLAB:** Built-in linear algebra functions
- **R:** Matrix package, eigen() function
- **Julia:** LinearAlgebra.jl package

